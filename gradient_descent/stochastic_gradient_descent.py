'''
Stochastic Gradient Descent uses a random data point in the data. From there, derivatives are calculated for the parameters.
Mini Batch Stochastic Gradient Descent uses several random values to converge on optimal values in fewer steps
'''

